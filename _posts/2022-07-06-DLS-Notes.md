---
layout: post
title: (Google ML Bootcamp LATAM) Notes on DLS Coursera
categories: [coding, ML]
---


## Course 1: NNs and DL

### Week 1: Introduction to DL.
Geoffrey Hinton proved in the 90s that ReLU is a good approximation for a logistic regression. That's  why ReLU works fine in DNNs.

### Week 2: NNs Basics

* Non-convex loss function minimum when using MSE for logistic
* Python Broadcasting is more powerful than expected haha. 
* Cost/Lost function: On the batch / on a single sample
* Using random init weights is not the meta, it was never the meta.
* Vectorizing even at batch updates? Yeah
* reshape is a constant time operation
* Don't use rank 1 vector in python, i.e (n,), instead use (n,1) or (1,n)
* Use `assert()` wisely to debug
* The use of MLE begins even more into the baby examples! The logistic regression is just a form of MLE using one layer only.
* (size_input, batch_size)

*  Nomenclature:



###  Week 3: Shallow NNs

* (hidden_units, batch_size)
* Avoid sigmoid for hidden layers, except for output for 0-1 output
* - At the beginning there was structured data

## Course 2: Improving DNNs: Hyperparameter Tuning, Regularization and Optimization

## Course 3: Structuring Machine Learning Projects

## Course 4: CNNs

